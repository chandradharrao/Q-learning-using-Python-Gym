{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "envi = gym.make(\"Taxi-v3\").env\n",
    "envi.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space Discrete(6)\n",
      "State space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action space {}\".format(envi.action_space))\n",
    "print(\"State space {}\".format(envi.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State :  456\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[42m_\u001b[0m|B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#we encode the state of environemtn wrt to the curr location of the taxi and end point\n",
    "state = envi.encode(4,2,4,0)\n",
    "print(\"State : \",state)\n",
    "#we set the state of environemnt manually with the encoded number\n",
    "envi.s = state\n",
    "envi.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of columns/actions = 6\n",
      "No of rows/state = 500\n"
     ]
    }
   ],
   "source": [
    "#when the Taxi env is created,a reward table is also created called as  \"P\"\n",
    "#It is a matrix with shape #states X #actions\n",
    "print(\"No of columns/actions = {}\".format(len(envi.P[0])))\n",
    "print(\"No of rows/state = {}\".format(len(envi.P)))  #(5*5)*(4+1)*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of timesteps taken 813\n",
      "Number of penalties incurred 233\n"
     ]
    }
   ],
   "source": [
    "#let us use brute force method to solve the problem\n",
    "#WITHOUT RL\n",
    "#we will create infinite loop until one passenger reaches one destination \n",
    "#ie untill reward = 20\n",
    "\n",
    "#current env state\n",
    "envi.s = 456\n",
    "\n",
    "epochs = 0\n",
    "penalties,reward = 0,0\n",
    "\n",
    "done = False #until passenger is dropped\n",
    "\n",
    "#list containing the details o each frame\n",
    "frames = []\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    #choose a random action from the 6 actions\n",
    "    action = envi.action_space.sample()\n",
    "    #collect info about what our actions are doing to the environemnt\n",
    "    #after performing the action\n",
    "    state,reward,done,info = envi.step(action)\n",
    "    \n",
    "    if reward == -10: #wrong pickup or drop actions\n",
    "        penalties +=1\n",
    "    #if done:\n",
    "        #print(\"epoch number {}\".format(epochs))\n",
    "        #print(\"No of frames {}\".format(len(frames)))\n",
    "        \n",
    "    currFrame = ({\n",
    "        \"frame\":envi.render(mode = \"ansi\"), #ansi graphic of the state\n",
    "        \"state\":state, #state number\n",
    "        \"action\":action, #what action caused it\n",
    "        \"reward\":reward  #reward of the action\n",
    "    })\n",
    "    frames.append(currFrame)\n",
    "\n",
    "    epochs +=1\n",
    "    \n",
    "print(\"Number of timesteps taken {}\".format(epochs))\n",
    "print(\"Number of penalties incurred {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first frame's graphic\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[42m_\u001b[0m|B: |\n",
      "+---------+\n",
      "  (East)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Each frame has a dictionary containing the frame graphics,\n",
    "#what action caused it to go to the particular state and\n",
    "#what was the reward in doing so\n",
    "\n",
    "#example\n",
    "print(\"The first frame's graphic\")\n",
    "print(frames[0][\"frame\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation not simulated\n"
     ]
    }
   ],
   "source": [
    "#create an animation playing all the frames until dropping the passenger off\n",
    "#we dont want to print one below the other,hence we clear the screen everytime\n",
    "#clearing the screen is like the refresh rate\n",
    "import IPython.display as jupyter\n",
    "import time \n",
    "\n",
    "simulate = False\n",
    "\n",
    "if (simulate):\n",
    "    for i in range(len(frames)):\n",
    "        jupyter.clear_output(wait = True)\n",
    "        time.sleep(0.1)\n",
    "        print(frames[i][\"frame\"])\n",
    "else:\n",
    "    print(\"Simulation not simulated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the above simulation was not good\n",
    "#the agent wasnt learning from its previuos steps and doesnt have memory of\n",
    "#its best state\n",
    "\n",
    "#Q-learning algorithm will give our agent some memory\n",
    "#the agent will use the environemnt's reward system to learn over time the best\n",
    "#action to take in a given state\n",
    "#what it does is that it compares the reward for an particluar action in a \n",
    "#particular state and sees if the action was benefecial\n",
    "#if the ction was benefecial,then it will remember it by updating the q-value table\n",
    "#the q-value table maps a particular state to the action taken in that state\n",
    "\n",
    "#in the q-learning algorithm,\n",
    "#alpha is the learning rate ie rate at which Q-values are being updated\n",
    "#gamma is the discount factor that gives the importance we want to give to long\n",
    "#term effective award rather than immediate reward\n",
    "\n",
    "#we update the Q-value with (1−α)Q(state,action) and then add the learned value\n",
    "#which is a combination of the reward for taking the current action in the current state and\n",
    "#discounted maximum reward from the next state we are going to be if we take the\n",
    "#current action.Its given by α(reward+γmaxaQ(next state,all actions)\n",
    "#here gamma controls the importance we are gonna give to the long term reward\n",
    "\n",
    "#hence we are learning the proper actions to take in the current state by looking\n",
    "#at the reward for the current state-action combo and the reward for the next state\n",
    "#hence our taxi will consider the best route\n",
    "#the number of rows in q-state: #states\n",
    "#number of columns = #actions\n",
    "\n",
    "#initially the Q-value are initialized to zero and then updated during the training\n",
    "#to the values that optimize the agent's traversal through the environment for\n",
    "#maximum reward\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
